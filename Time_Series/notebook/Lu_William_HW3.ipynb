{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4d4f6a",
   "metadata": {},
   "source": [
    "# This is homework 3 of Machine Learning\n",
    "### Classification of Time Series\n",
    "#### William Lu\n",
    "#### 7424831487\n",
    "#### WilliamLu1999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4387aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up and download\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3dc6c",
   "metadata": {},
   "source": [
    "##### 1 (a) (b) Process of Data\n",
    "Choosing the test and train data\n",
    "\n",
    "Need to skip the first 4 rows as they are empty and interruptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0a3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bending1_path=['../data/Arem/bending1/dataset1.csv','../data/Arem/bending1/dataset2.csv','../data/Arem/bending1/dataset3.csv',\n",
    "               '../data/Arem/bending1/dataset4.csv','../data/Arem/bending1/dataset5.csv','../data/Arem/bending1/dataset6.csv',\n",
    "               '../data/Arem/bending1/dataset7.csv']\n",
    "bending1_df=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2e7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in bending1_path:\n",
    "    bending1_df.append(pd.read_csv(file,skiprows=4))\n",
    "final_bending1 = pd.concat(bending1_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bbfa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bending1['label']='bending1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e271fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bending2_path = ['../data/Arem/bending2/dataset1.csv','../data/Arem/bending2/dataset2.csv','../data/Arem/bending2/dataset3.csv','../data/Arem/bending2/dataset5.csv','../data/Arem/bending2/dataset6.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde48531",
   "metadata": {},
   "outputs": [],
   "source": [
    "bending2_df=[]\n",
    "for file in bending2_path:\n",
    "    bending2_df.append(pd.read_csv(file,skiprows=4))\n",
    "# add dataset 4 separately since it is corrupted\n",
    "bending2_df.append(pd.read_csv('../data/Arem/bending2/dataset4.csv',sep=' ',names=['# Columns: time',\n",
    "                                                                'avg_rss12',\n",
    "                                                                 'var_rss12','avg_rss13',\n",
    "                                                                 'var_rss13','avg_rss23',\n",
    "                                                                 'var_rss23'],skiprows=5,index_col=False))\n",
    "## !!! we need to concat again because dataset 4 in bending2 folder has bad separator ' '.\n",
    "## we need to concatenate them at the end of bending2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a6f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bending2 = pd.concat(bending2_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9149ebcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_bending2['label']='bending2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d58b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the last comma in dataset9 and 14 of cycling by hands already\n",
    "cycling_path=['../data/Arem/cycling/dataset1.csv','../data/Arem/cycling/dataset2.csv','../data/Arem/cycling/dataset3.csv',\n",
    "             '../data/Arem/cycling/dataset4.csv','../data/Arem/cycling/dataset5.csv','../data/Arem/cycling/dataset6.csv',\n",
    "             '../data/Arem/cycling/dataset7.csv','../data/Arem/cycling/dataset8.csv','../data/Arem/cycling/dataset9.csv',\n",
    "             '../data/Arem/cycling/dataset10.csv','../data/Arem/cycling/dataset11.csv','../data/Arem/cycling/dataset12.csv',\n",
    "             '../data/Arem/cycling/dataset13.csv','../data/Arem/cycling/dataset14.csv','../data/Arem/cycling/dataset14.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8539972",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycling_df=[]\n",
    "for file in cycling_path:\n",
    "    cycling_df.append(pd.read_csv(file,skiprows=4))\n",
    "final_cycling = pd.concat(cycling_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f561f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cycling['label']='cycling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ba2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "lying_path=['../data/Arem/lying/dataset1.csv','../data/Arem/lying/dataset2.csv','../data/Arem/lying/dataset3.csv',\n",
    "             '../data/Arem/lying/dataset4.csv','../data/Arem/lying/dataset5.csv','../data/Arem/lying/dataset6.csv',\n",
    "             '../data/Arem/lying/dataset7.csv','../data/Arem/lying/dataset8.csv','../data/Arem/lying/dataset9.csv',\n",
    "             '../data/Arem/lying/dataset10.csv','../data/Arem/lying/dataset11.csv','../data/Arem/lying/dataset12.csv',\n",
    "             '../data/Arem/lying/dataset13.csv','../data/Arem/lying/dataset14.csv','../data/Arem/lying/dataset15.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377a1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "lying_df=[]\n",
    "for file in lying_path:\n",
    "    lying_df.append(pd.read_csv(file,skiprows=4))\n",
    "final_lying = pd.concat(lying_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8048268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lying['label']='lying'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568b26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitting_path=['../data/Arem/sitting/dataset1.csv','../data/Arem/sitting/dataset2.csv','../data/Arem/sitting/dataset3.csv',\n",
    "             '../data/Arem/sitting/dataset4.csv','../data/Arem/sitting/dataset5.csv','../data/Arem/sitting/dataset6.csv',\n",
    "             '../data/Arem/sitting/dataset7.csv','../data/Arem/sitting/dataset8.csv','../data/Arem/sitting/dataset9.csv',\n",
    "             '../data/Arem/sitting/dataset10.csv','../data/Arem/sitting/dataset11.csv','../data/Arem/sitting/dataset12.csv',\n",
    "             '../data/Arem/sitting/dataset13.csv','../data/Arem/sitting/dataset14.csv','../data/Arem/sitting/dataset15.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10416a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitting_df=[]\n",
    "for file in sitting_path:\n",
    "    sitting_df.append(pd.read_csv(file,skiprows=4))\n",
    "final_sitting = pd.concat(sitting_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb7974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sitting['label']='sitting'\n",
    "# final_sitting has 1 missing row.\n",
    "# final_sitting.loc[final_sitting.avg_rss12==None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a6f61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "standing_path=['../data/Arem/standing/dataset1.csv','../data/Arem/standing/dataset2.csv','../data/Arem/standing/dataset3.csv',\n",
    "             '../data/Arem/standing/dataset4.csv','../data/Arem/standing/dataset5.csv','../data/Arem/standing/dataset6.csv',\n",
    "             '../data/Arem/standing/dataset7.csv','../data/Arem/standing/dataset8.csv','../data/Arem/standing/dataset9.csv',\n",
    "             '../data/Arem/standing/dataset10.csv','../data/Arem/standing/dataset11.csv','../data/Arem/standing/dataset12.csv',\n",
    "             '../data/Arem/standing/dataset13.csv','../data/Arem/standing/dataset14.csv','../data/Arem/standing/dataset15.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49886402",
   "metadata": {},
   "outputs": [],
   "source": [
    "standing_df=[]\n",
    "for file in standing_path:\n",
    "    standing_df.append(pd.read_csv(file,skiprows=4))\n",
    "final_standing = pd.concat(standing_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d164f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_standing['label']='standing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40fa9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "walking_path=['../data/Arem/walking/dataset1.csv','../data/Arem/walking/dataset2.csv','../data/Arem/walking/dataset3.csv',\n",
    "             '../data/Arem/walking/dataset4.csv','../data/Arem/walking/dataset5.csv','../data/Arem/walking/dataset6.csv',\n",
    "             '../data/Arem/walking/dataset7.csv','../data/Arem/walking/dataset8.csv','../data/Arem/walking/dataset9.csv',\n",
    "             '../data/Arem/walking/dataset10.csv','../data/Arem/walking/dataset11.csv','../data/Arem/walking/dataset12.csv',\n",
    "             '../data/Arem/walking/dataset13.csv','../data/Arem/walking/dataset14.csv','../data/Arem/walking/dataset15.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6d15d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "walking_df=[]\n",
    "for file in walking_path:\n",
    "    walking_df.append(pd.read_csv(file,skiprows=4))\n",
    "final_walking = pd.concat(walking_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de2bad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_walking['label']='walking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1e6e1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Columns: time</th>\n",
       "      <th>avg_rss12</th>\n",
       "      <th>var_rss12</th>\n",
       "      <th>avg_rss13</th>\n",
       "      <th>var_rss13</th>\n",
       "      <th>avg_rss23</th>\n",
       "      <th>var_rss23</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>22.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>33.75</td>\n",
       "      <td>1.30</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>39.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>39.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>23.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>39.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>39.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9115</th>\n",
       "      <td>118750</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>17.00</td>\n",
       "      <td>5.10</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.87</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9116</th>\n",
       "      <td>119000</td>\n",
       "      <td>34.33</td>\n",
       "      <td>1.89</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9117</th>\n",
       "      <td>119250</td>\n",
       "      <td>33.00</td>\n",
       "      <td>7.35</td>\n",
       "      <td>14.60</td>\n",
       "      <td>3.14</td>\n",
       "      <td>13.00</td>\n",
       "      <td>5.70</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>119500</td>\n",
       "      <td>31.67</td>\n",
       "      <td>1.25</td>\n",
       "      <td>11.00</td>\n",
       "      <td>6.16</td>\n",
       "      <td>19.25</td>\n",
       "      <td>2.17</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9119</th>\n",
       "      <td>119750</td>\n",
       "      <td>30.75</td>\n",
       "      <td>10.21</td>\n",
       "      <td>11.75</td>\n",
       "      <td>1.09</td>\n",
       "      <td>18.50</td>\n",
       "      <td>3.20</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
       "0                   0      39.25       0.43      22.75       0.43      33.75   \n",
       "1                 250      39.25       0.43      23.00       0.00      33.00   \n",
       "2                 500      39.25       0.43      23.25       0.43      33.00   \n",
       "3                 750      39.50       0.50      23.00       0.71      33.00   \n",
       "4                1000      39.50       0.50      24.00       0.00      33.00   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "9115           118750      36.00       2.45      17.00       5.10      20.50   \n",
       "9116           119000      34.33       1.89      15.00       2.45      17.00   \n",
       "9117           119250      33.00       7.35      14.60       3.14      13.00   \n",
       "9118           119500      31.67       1.25      11.00       6.16      19.25   \n",
       "9119           119750      30.75      10.21      11.75       1.09      18.50   \n",
       "\n",
       "      var_rss23     label  \n",
       "0          1.30  bending1  \n",
       "1          0.00  bending1  \n",
       "2          0.00  bending1  \n",
       "3          0.00  bending1  \n",
       "4          0.00  bending1  \n",
       "...         ...       ...  \n",
       "9115       0.87   walking  \n",
       "9116       2.12   walking  \n",
       "9117       5.70   walking  \n",
       "9118       2.17   walking  \n",
       "9119       3.20   walking  \n",
       "\n",
       "[9120 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine training data from all dataframes\n",
    "# since each dataset contain 480 rows, so we need to select 960 rows for bending1 and bending2 as training data\n",
    "bending1_test = final_bending1.iloc[0:960,::] \n",
    "bending2_test = final_bending2.iloc[0:960,::]\n",
    "# first three datasets of the remaining folders will be test data, so 1440 rows\n",
    "cycling_test = final_cycling.iloc[0:1440,::]\n",
    "lying_test = final_lying.iloc[0:1440,::]\n",
    "sitting_test = final_sitting.iloc[0:1440,::]\n",
    "standing_test = final_standing.iloc[0:1440,::]\n",
    "walking_test = final_walking.iloc[0:1440,::]\n",
    "test_data = pd.concat([bending1_test,bending2_test,cycling_test,lying_test,sitting_test,standing_test,walking_test],axis=0,ignore_index=True)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "245e8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we get the training data from the remaining rows of each dataset\n",
    "bending1_train = final_bending1.iloc[960::,::] \n",
    "bending2_train = final_bending2.iloc[960::,::]\n",
    "cycling_train = final_cycling.iloc[1440::,::]\n",
    "lying_train = final_lying.iloc[1440::,::]\n",
    "sitting_train = final_sitting.iloc[1440::,::]\n",
    "standing_train = final_standing.iloc[1440::,::]\n",
    "walking_train = final_walking.iloc[1440::,::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8b20b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Columns: time</th>\n",
       "      <th>avg_rss12</th>\n",
       "      <th>var_rss12</th>\n",
       "      <th>avg_rss13</th>\n",
       "      <th>var_rss13</th>\n",
       "      <th>avg_rss23</th>\n",
       "      <th>var_rss23</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>21.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>41.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>20.25</td>\n",
       "      <td>1.48</td>\n",
       "      <td>31.25</td>\n",
       "      <td>1.09</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>41.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>14.25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>40.75</td>\n",
       "      <td>0.83</td>\n",
       "      <td>15.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>20.00</td>\n",
       "      <td>2.74</td>\n",
       "      <td>32.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>bending1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33114</th>\n",
       "      <td>118750</td>\n",
       "      <td>34.50</td>\n",
       "      <td>6.18</td>\n",
       "      <td>9.00</td>\n",
       "      <td>3.56</td>\n",
       "      <td>12.67</td>\n",
       "      <td>4.19</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33115</th>\n",
       "      <td>119000</td>\n",
       "      <td>25.75</td>\n",
       "      <td>6.02</td>\n",
       "      <td>13.75</td>\n",
       "      <td>2.05</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33116</th>\n",
       "      <td>119250</td>\n",
       "      <td>31.50</td>\n",
       "      <td>3.35</td>\n",
       "      <td>10.25</td>\n",
       "      <td>5.12</td>\n",
       "      <td>16.25</td>\n",
       "      <td>2.95</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33117</th>\n",
       "      <td>119500</td>\n",
       "      <td>33.75</td>\n",
       "      <td>2.77</td>\n",
       "      <td>14.00</td>\n",
       "      <td>3.24</td>\n",
       "      <td>13.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33118</th>\n",
       "      <td>119750</td>\n",
       "      <td>37.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>18.25</td>\n",
       "      <td>3.70</td>\n",
       "      <td>11.00</td>\n",
       "      <td>4.32</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33119 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
       "0                    0      42.00       0.71      21.25       0.43      30.00   \n",
       "1                  250      41.50       0.50      20.25       1.48      31.25   \n",
       "2                  500      41.50       0.50      14.25       1.92      33.00   \n",
       "3                  750      40.75       0.83      15.75       0.43      33.00   \n",
       "4                 1000      40.00       0.71      20.00       2.74      32.75   \n",
       "...                ...        ...        ...        ...        ...        ...   \n",
       "33114           118750      34.50       6.18       9.00       3.56      12.67   \n",
       "33115           119000      25.75       6.02      13.75       2.05      16.00   \n",
       "33116           119250      31.50       3.35      10.25       5.12      16.25   \n",
       "33117           119500      33.75       2.77      14.00       3.24      13.75   \n",
       "33118           119750      37.00       1.41      18.25       3.70      11.00   \n",
       "\n",
       "       var_rss23     label  \n",
       "0           0.00  bending1  \n",
       "1           1.09  bending1  \n",
       "2           0.00  bending1  \n",
       "3           0.00  bending1  \n",
       "4           0.43  bending1  \n",
       "...          ...       ...  \n",
       "33114       4.19   walking  \n",
       "33115       1.58   walking  \n",
       "33116       2.95   walking  \n",
       "33117       0.43   walking  \n",
       "33118       4.32   walking  \n",
       "\n",
       "[33119 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat([bending1_train,bending2_train,cycling_train,lying_train,sitting_train,standing_train,walking_train],axis=0,ignore_index=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5bb4a9",
   "metadata": {},
   "source": [
    "##### 1 (c) i.  What types of time-domain features are usually used in time series classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a2162",
   "metadata": {},
   "source": [
    "Minimum, maximum, mean, median, standard deviation, first quartile, third quartile, stationarity, entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820c39c",
   "metadata": {},
   "source": [
    "##### 1(c) ii Extract time domain features for each of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57e52e",
   "metadata": {},
   "source": [
    "We have 88 datasets. For each dataset, each column needs to be calculated for min, max, mean, median, standard deviation, first quartile, and third quartile. And there are 6 columns need to be done for this for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf26ae63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bending1_table=[]\n",
    "bending1_f = []\n",
    "for file in bending1_path:\n",
    "    bending1_items =pd.read_csv(file,skiprows=4).iloc[::,1:7].describe()\n",
    "    bending1_table.append(bending1_items[1::][::]) # getting rid of count\n",
    "for f in bending1_table:\n",
    "    f = f.reindex(['min','max','mean','50%','std','25%','75%'])\n",
    "    f = f.rename({'50%':'median','25%':'first quartile','75%':'third quartile','std':'standard deviation'})\n",
    "    bending1_f.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "048a6d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr=[]\n",
    "for f in bending1_f:\n",
    "    f= list(f.to_numpy().T.flatten()) # f is the describe table of each dataset\n",
    "    arr.append(f)\n",
    "sum_df = pd.DataFrame(columns=['min_1', 'max_1','mean_1','median_1','standard deviation_1','1st quart_1','3rd quart_1',\n",
    "                              'min_2', 'max_2','mean_2','median_2','standard deviation_2','1st quart_2','3rd quart_2',\n",
    "                              'min_3', 'max_3','mean_3','median_3','standard deviation_3','1st quart_3','3rd quart_3',\n",
    "                              'min_4', 'max_4','mean_4','median_4','standard deviation_4','1st quart_4','3rd quart_4',\n",
    "                              'min_5', 'max_5','mean_5','median_5','standard deviation_5','1st quart_5','3rd quart_5',\n",
    "                              'min_6', 'max_6','mean_6','median_6','standard deviation_6','1st quart_6','3rd quart_6'], data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f31382b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bending2_table=[]\n",
    "bending2_f = []\n",
    "for file in bending2_path:\n",
    "    bending2_items =pd.read_csv(file,skiprows=4).iloc[::,1:7].describe()\n",
    "    bending2_table.append(bending2_items[1::][::]) # getting rid of count\n",
    "# we need to append the dataset 4 of the bending2 folder...\n",
    "bending2_d4 =pd.read_csv('../data/Arem/bending2/dataset4.csv',sep=' ',names=['# Columns: time',\n",
    "                                                                'avg_rss12',\n",
    "                                                                 'var_rss12','avg_rss13',\n",
    "                                                                 'var_rss13','avg_rss23',\n",
    "                                                                 'var_rss23'],skiprows=5,index_col=False).iloc[::,1:7].describe()\n",
    "bending2_table.append(bending2_d4[1::][::])\n",
    "for f in bending2_table:\n",
    "    f = f.reindex(['min','max','mean','50%','std','25%','75%'])\n",
    "    f = f.rename({'50%':'median','25%':'first quartile','75%':'third quartile','std':'standard deviation'})\n",
    "    bending2_f.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1e17f81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arr_b2=[]\n",
    "for f in bending2_f:\n",
    "    f= list(f.to_numpy().T.flatten()) # f is the describe table of each dataset\n",
    "    arr_b2.append(f)\n",
    "#arr_b2=pd.DataFrame(data=arr_b2)\n",
    "#sum_df=pd.concat([sum_df, arr_b2], ignore_index=True,axis=0)\n",
    "sum_df2 = pd.DataFrame(columns=['min_1', 'max_1','mean_1','median_1','standard deviation_1','1st quart_1','3rd quart_1',\n",
    "                              'min_2', 'max_2','mean_2','median_2','standard deviation_2','1st quart_2','3rd quart_2',\n",
    "                              'min_3', 'max_3','mean_3','median_3','standard deviation_3','1st quart_3','3rd quart_3',\n",
    "                              'min_4', 'max_4','mean_4','median_4','standard deviation_4','1st quart_4','3rd quart_4',\n",
    "                              'min_5', 'max_5','mean_5','median_5','standard deviation_5','1st quart_5','3rd quart_5',\n",
    "                              'min_6', 'max_6','mean_6','median_6','standard deviation_6','1st quart_6','3rd quart_6'], data=arr_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e444d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycling_table=[]\n",
    "cycling_f = []\n",
    "for file in cycling_path:\n",
    "    cycling_items =pd.read_csv(file,skiprows=4).iloc[::,1:7].describe()\n",
    "    cycling_table.append(cycling_items[1::][::]) # getting rid of count\n",
    "for f in cycling_table:\n",
    "    f = f.reindex(['min','max','mean','50%','std','25%','75%'])\n",
    "    f = f.rename({'50%':'median','25%':'first quartile','75%':'third quartile','std':'standard deviation'})\n",
    "    cycling_f.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca879dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_cycling=[]\n",
    "for f in cycling_f:\n",
    "    f= list(f.to_numpy().T.flatten()) # f is the describe table of each dataset\n",
    "    arr_cycling.append(f)\n",
    "sum_df3 = pd.DataFrame(columns=['min_1', 'max_1','mean_1','median_1','standard deviation_1','1st quart_1','3rd quart_1',\n",
    "                              'min_2', 'max_2','mean_2','median_2','standard deviation_2','1st quart_2','3rd quart_2',\n",
    "                              'min_3', 'max_3','mean_3','median_3','standard deviation_3','1st quart_3','3rd quart_3',\n",
    "                              'min_4', 'max_4','mean_4','median_4','standard deviation_4','1st quart_4','3rd quart_4',\n",
    "                              'min_5', 'max_5','mean_5','median_5','standard deviation_5','1st quart_5','3rd quart_5',\n",
    "                              'min_6', 'max_6','mean_6','median_6','standard deviation_6','1st quart_6','3rd quart_6'], data=arr_cycling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00258043",
   "metadata": {},
   "outputs": [],
   "source": [
    "lying_table=[]\n",
    "lying_f = []\n",
    "for file in lying_path:\n",
    "    lying_items =pd.read_csv(file,skiprows=4).iloc[::,1:7].describe()\n",
    "    lying_table.append(lying_items[1::][::]) # getting rid of count\n",
    "for f in lying_table:\n",
    "    f = f.reindex(['min','max','mean','50%','std','25%','75%'])\n",
    "    f = f.rename({'50%':'median','25%':'first quartile','75%':'third quartile','std':'standard deviation'})\n",
    "    lying_f.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46ef5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_lying=[]\n",
    "for f in lying_f:\n",
    "    f= list(f.to_numpy().T.flatten()) # f is the describe table of each dataset\n",
    "    arr_lying.append(f)\n",
    "sum_df4 = pd.DataFrame(columns=['min_1', 'max_1','mean_1','median_1','standard deviation_1','1st quart_1','3rd quart_1',\n",
    "                              'min_2', 'max_2','mean_2','median_2','standard deviation_2','1st quart_2','3rd quart_2',\n",
    "                              'min_3', 'max_3','mean_3','median_3','standard deviation_3','1st quart_3','3rd quart_3',\n",
    "                              'min_4', 'max_4','mean_4','median_4','standard deviation_4','1st quart_4','3rd quart_4',\n",
    "                              'min_5', 'max_5','mean_5','median_5','standard deviation_5','1st quart_5','3rd quart_5',\n",
    "                              'min_6', 'max_6','mean_6','median_6','standard deviation_6','1st quart_6','3rd quart_6'], data=arr_lying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b4716fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitting_table=[]\n",
    "sitting_f = []\n",
    "for file in sitting_path:\n",
    "    sitting_items =pd.read_csv(file,skiprows=4).iloc[::,1:7].describe()\n",
    "    sitting_table.append(sitting_items[1::][::]) # getting rid of count\n",
    "for f in sitting_table:\n",
    "    f = f.reindex(['min','max','mean','50%','std','25%','75%'])\n",
    "    f = f.rename({'50%':'median','25%':'first quartile','75%':'third quartile','std':'standard deviation'})\n",
    "    sitting_f.append(f)\n",
    "arr_sitting=[]\n",
    "for f in sitting_f:\n",
    "    f= list(f.to_numpy().T.flatten()) # f is the describe table of each dataset\n",
    "    arr_sitting.append(f)\n",
    "sum_df5 = pd.DataFrame(columns=['min_1', 'max_1','mean_1','median_1','standard deviation_1','1st quart_1','3rd quart_1',\n",
    "                              'min_2', 'max_2','mean_2','median_2','standard deviation_2','1st quart_2','3rd quart_2',\n",
    "                              'min_3', 'max_3','mean_3','median_3','standard deviation_3','1st quart_3','3rd quart_3',\n",
    "                              'min_4', 'max_4','mean_4','median_4','standard deviation_4','1st quart_4','3rd quart_4',\n",
    "                              'min_5', 'max_5','mean_5','median_5','standard deviation_5','1st quart_5','3rd quart_5',\n",
    "                              'min_6', 'max_6','mean_6','median_6','standard deviation_6','1st quart_6','3rd quart_6'], data=arr_sitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24352d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "standing_table=[]\n",
    "standing_f = []\n",
    "for file in standing_path:\n",
    "    standing_items =pd.read_csv(file,skiprows=4).iloc[::,1:7].describe()\n",
    "    standing_table.append(standing_items[1::][::]) # getting rid of count\n",
    "for f in standing_table:\n",
    "    f = f.reindex(['min','max','mean','50%','std','25%','75%'])\n",
    "    f = f.rename({'50%':'median','25%':'first quartile','75%':'third quartile','std':'standard deviation'})\n",
    "    standing_f.append(f)\n",
    "arr_standing=[]\n",
    "for f in standing_f:\n",
    "    f= list(f.to_numpy().T.flatten()) # f is the describe table of each dataset\n",
    "    arr_standing.append(f)\n",
    "sum_df6 = pd.DataFrame(columns=['min_1', 'max_1','mean_1','median_1','standard deviation_1','1st quart_1','3rd quart_1',\n",
    "                              'min_2', 'max_2','mean_2','median_2','standard deviation_2','1st quart_2','3rd quart_2',\n",
    "                              'min_3', 'max_3','mean_3','median_3','standard deviation_3','1st quart_3','3rd quart_3',\n",
    "                              'min_4', 'max_4','mean_4','median_4','standard deviation_4','1st quart_4','3rd quart_4',\n",
    "                              'min_5', 'max_5','mean_5','median_5','standard deviation_5','1st quart_5','3rd quart_5',\n",
    "                              'min_6', 'max_6','mean_6','median_6','standard deviation_6','1st quart_6','3rd quart_6'], data=arr_standing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd57f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "walking_table=[]\n",
    "walking_f = []\n",
    "for file in walking_path:\n",
    "    walking_items =pd.read_csv(file,skiprows=4).iloc[::,1:7].describe()\n",
    "    walking_table.append(walking_items[1::][::]) # getting rid of count\n",
    "for f in walking_table:\n",
    "    f = f.reindex(['min','max','mean','50%','std','25%','75%'])\n",
    "    f = f.rename({'50%':'median','25%':'first quartile','75%':'third quartile','std':'standard deviation'})\n",
    "    walking_f.append(f)\n",
    "arr_walking=[]\n",
    "for f in walking_f:\n",
    "    f= list(f.to_numpy().T.flatten()) # f is the describe table of each dataset\n",
    "    arr_walking.append(f)\n",
    "sum_df7 = pd.DataFrame(columns=['min_1', 'max_1','mean_1','median_1','standard deviation_1','1st quart_1','3rd quart_1',\n",
    "                              'min_2', 'max_2','mean_2','median_2','standard deviation_2','1st quart_2','3rd quart_2',\n",
    "                              'min_3', 'max_3','mean_3','median_3','standard deviation_3','1st quart_3','3rd quart_3',\n",
    "                              'min_4', 'max_4','mean_4','median_4','standard deviation_4','1st quart_4','3rd quart_4',\n",
    "                              'min_5', 'max_5','mean_5','median_5','standard deviation_5','1st quart_5','3rd quart_5',\n",
    "                              'min_6', 'max_6','mean_6','median_6','standard deviation_6','1st quart_6','3rd quart_6'], data=arr_walking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6325e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>min_1</th>\n",
       "      <th>max_1</th>\n",
       "      <th>mean_1</th>\n",
       "      <th>median_1</th>\n",
       "      <th>standard deviation_1</th>\n",
       "      <th>1st quart_1</th>\n",
       "      <th>3rd quart_1</th>\n",
       "      <th>min_2</th>\n",
       "      <th>max_2</th>\n",
       "      <th>...</th>\n",
       "      <th>standard deviation_5</th>\n",
       "      <th>1st quart_5</th>\n",
       "      <th>3rd quart_5</th>\n",
       "      <th>min_6</th>\n",
       "      <th>max_6</th>\n",
       "      <th>mean_6</th>\n",
       "      <th>median_6</th>\n",
       "      <th>standard deviation_6</th>\n",
       "      <th>1st quart_6</th>\n",
       "      <th>3rd quart_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>37.25</td>\n",
       "      <td>45.00</td>\n",
       "      <td>40.624792</td>\n",
       "      <td>40.50</td>\n",
       "      <td>1.476967</td>\n",
       "      <td>39.25</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>...</td>\n",
       "      <td>2.188449</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.570583</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.00</td>\n",
       "      <td>45.67</td>\n",
       "      <td>42.812812</td>\n",
       "      <td>42.50</td>\n",
       "      <td>1.435550</td>\n",
       "      <td>42.00</td>\n",
       "      <td>43.6700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995255</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>34.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.571083</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.601010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>35.00</td>\n",
       "      <td>47.40</td>\n",
       "      <td>43.954500</td>\n",
       "      <td>44.33</td>\n",
       "      <td>1.558835</td>\n",
       "      <td>43.00</td>\n",
       "      <td>45.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.999604</td>\n",
       "      <td>35.3625</td>\n",
       "      <td>36.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.493292</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.513506</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33.00</td>\n",
       "      <td>47.75</td>\n",
       "      <td>42.179813</td>\n",
       "      <td>43.50</td>\n",
       "      <td>3.670666</td>\n",
       "      <td>39.15</td>\n",
       "      <td>45.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.849448</td>\n",
       "      <td>30.4575</td>\n",
       "      <td>36.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.613521</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.524317</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33.00</td>\n",
       "      <td>45.75</td>\n",
       "      <td>41.678063</td>\n",
       "      <td>41.75</td>\n",
       "      <td>2.243490</td>\n",
       "      <td>41.33</td>\n",
       "      <td>42.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>...</td>\n",
       "      <td>2.411026</td>\n",
       "      <td>28.4575</td>\n",
       "      <td>31.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.383292</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.389164</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>19.50</td>\n",
       "      <td>45.33</td>\n",
       "      <td>33.586875</td>\n",
       "      <td>34.25</td>\n",
       "      <td>4.650935</td>\n",
       "      <td>30.25</td>\n",
       "      <td>37.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.67</td>\n",
       "      <td>...</td>\n",
       "      <td>3.283983</td>\n",
       "      <td>13.7300</td>\n",
       "      <td>18.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.32</td>\n",
       "      <td>3.259729</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.640243</td>\n",
       "      <td>2.0500</td>\n",
       "      <td>4.3225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>19.75</td>\n",
       "      <td>45.50</td>\n",
       "      <td>34.322750</td>\n",
       "      <td>35.25</td>\n",
       "      <td>4.752477</td>\n",
       "      <td>31.00</td>\n",
       "      <td>38.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.47</td>\n",
       "      <td>...</td>\n",
       "      <td>3.119856</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>17.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.67</td>\n",
       "      <td>3.432562</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.732727</td>\n",
       "      <td>2.1575</td>\n",
       "      <td>4.5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>19.50</td>\n",
       "      <td>46.00</td>\n",
       "      <td>34.546229</td>\n",
       "      <td>35.25</td>\n",
       "      <td>4.842294</td>\n",
       "      <td>31.25</td>\n",
       "      <td>37.8125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.47</td>\n",
       "      <td>...</td>\n",
       "      <td>2.823124</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>17.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3.338125</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.656742</td>\n",
       "      <td>2.1600</td>\n",
       "      <td>4.3350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>23.50</td>\n",
       "      <td>46.25</td>\n",
       "      <td>34.873229</td>\n",
       "      <td>35.25</td>\n",
       "      <td>4.531720</td>\n",
       "      <td>31.75</td>\n",
       "      <td>38.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.82</td>\n",
       "      <td>...</td>\n",
       "      <td>3.131076</td>\n",
       "      <td>13.7500</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.51</td>\n",
       "      <td>3.424646</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.690960</td>\n",
       "      <td>2.1700</td>\n",
       "      <td>4.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>19.25</td>\n",
       "      <td>44.00</td>\n",
       "      <td>34.473188</td>\n",
       "      <td>35.00</td>\n",
       "      <td>4.796705</td>\n",
       "      <td>31.25</td>\n",
       "      <td>38.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.86</td>\n",
       "      <td>...</td>\n",
       "      <td>3.156320</td>\n",
       "      <td>13.7300</td>\n",
       "      <td>17.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>9.00</td>\n",
       "      <td>3.340458</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1.699114</td>\n",
       "      <td>2.1200</td>\n",
       "      <td>4.3750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Instance  min_1  max_1     mean_1  median_1  standard deviation_1  \\\n",
       "0          1  37.25  45.00  40.624792     40.50              1.476967   \n",
       "1          2  38.00  45.67  42.812812     42.50              1.435550   \n",
       "2          3  35.00  47.40  43.954500     44.33              1.558835   \n",
       "3          4  33.00  47.75  42.179813     43.50              3.670666   \n",
       "4          5  33.00  45.75  41.678063     41.75              2.243490   \n",
       "..       ...    ...    ...        ...       ...                   ...   \n",
       "83        84  19.50  45.33  33.586875     34.25              4.650935   \n",
       "84        85  19.75  45.50  34.322750     35.25              4.752477   \n",
       "85        86  19.50  46.00  34.546229     35.25              4.842294   \n",
       "86        87  23.50  46.25  34.873229     35.25              4.531720   \n",
       "87        88  19.25  44.00  34.473188     35.00              4.796705   \n",
       "\n",
       "    1st quart_1  3rd quart_1  min_2  max_2  ...  standard deviation_5  \\\n",
       "0         39.25      42.0000    0.0   1.30  ...              2.188449   \n",
       "1         42.00      43.6700    0.0   1.22  ...              1.995255   \n",
       "2         43.00      45.0000    0.0   1.70  ...              1.999604   \n",
       "3         39.15      45.0000    0.0   3.00  ...              3.849448   \n",
       "4         41.33      42.7500    0.0   2.83  ...              2.411026   \n",
       "..          ...          ...    ...    ...  ...                   ...   \n",
       "83        30.25      37.0000    0.0  14.67  ...              3.283983   \n",
       "84        31.00      38.0000    0.0  13.47  ...              3.119856   \n",
       "85        31.25      37.8125    0.0  12.47  ...              2.823124   \n",
       "86        31.75      38.2500    0.0  14.82  ...              3.131076   \n",
       "87        31.25      38.0000    0.0  13.86  ...              3.156320   \n",
       "\n",
       "    1st quart_5  3rd quart_5  min_6  max_6    mean_6  median_6  \\\n",
       "0       33.0000        36.00   0.00   1.92  0.570583      0.43   \n",
       "1       32.0000        34.50   0.00   3.11  0.571083      0.43   \n",
       "2       35.3625        36.50   0.00   1.79  0.493292      0.43   \n",
       "3       30.4575        36.33   0.00   2.18  0.613521      0.50   \n",
       "4       28.4575        31.25   0.00   1.79  0.383292      0.43   \n",
       "..          ...          ...    ...    ...       ...       ...   \n",
       "83      13.7300        18.25   0.00   8.32  3.259729      3.11   \n",
       "84      13.5000        17.75   0.00   9.67  3.432562      3.20   \n",
       "85      14.0000        17.75   0.00  10.00  3.338125      3.08   \n",
       "86      13.7500        18.00   0.00   9.51  3.424646      3.27   \n",
       "87      13.7300        17.75   0.43   9.00  3.340458      3.09   \n",
       "\n",
       "    standard deviation_6  1st quart_6  3rd quart_6  \n",
       "0               0.582915       0.0000       1.3000  \n",
       "1               0.601010       0.0000       1.3000  \n",
       "2               0.513506       0.0000       0.9400  \n",
       "3               0.524317       0.0000       1.0000  \n",
       "4               0.389164       0.0000       0.5000  \n",
       "..                   ...          ...          ...  \n",
       "83              1.640243       2.0500       4.3225  \n",
       "84              1.732727       2.1575       4.5650  \n",
       "85              1.656742       2.1600       4.3350  \n",
       "86              1.690960       2.1700       4.5000  \n",
       "87              1.699114       2.1200       4.3750  \n",
       "\n",
       "[88 rows x 43 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_df = pd.concat([sum_df,sum_df2,sum_df3,sum_df4,sum_df5,sum_df6,sum_df7],ignore_index=True)\n",
    "df_df.insert(loc=0,column='Instance',value = 1+np.arange(len(df_df))) # make it right\n",
    "df_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed373f8",
   "metadata": {},
   "source": [
    "##### 1 (c) iii standard deviation and confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "925c910d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Instance                25.547342\n",
       "min_1                    9.575765\n",
       "max_1                    4.399124\n",
       "mean_1                   5.340140\n",
       "median_1                 5.437561\n",
       "standard deviation_1     1.773264\n",
       "1st quart_1              6.159497\n",
       "3rd quart_1              5.142148\n",
       "min_2                    0.000000\n",
       "max_2                    5.070032\n",
       "mean_2                   1.577051\n",
       "median_2                 1.413781\n",
       "standard deviation_2     0.889548\n",
       "1st quart_2              0.945886\n",
       "3rd quart_2              2.130063\n",
       "min_3                    2.974717\n",
       "max_3                    4.894919\n",
       "mean_3                   4.025789\n",
       "median_3                 4.049974\n",
       "standard deviation_3     0.947750\n",
       "1st quart_3              4.231631\n",
       "3rd quart_3              4.189527\n",
       "min_4                    0.000000\n",
       "max_4                    2.203062\n",
       "mean_4                   1.169386\n",
       "median_4                 1.149559\n",
       "standard deviation_4     0.461591\n",
       "1st quart_4              0.845434\n",
       "3rd quart_4              1.557458\n",
       "min_5                    6.127339\n",
       "max_5                    5.747005\n",
       "mean_5                   5.668792\n",
       "median_5                 5.804823\n",
       "standard deviation_5     1.022547\n",
       "1st quart_5              6.083860\n",
       "3rd quart_5              5.527275\n",
       "min_6                    0.045838\n",
       "max_6                    2.549891\n",
       "mean_6                   1.164478\n",
       "median_6                 1.095190\n",
       "standard deviation_6     0.519732\n",
       "1st quart_6              0.767959\n",
       "3rd quart_6              1.534989\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard deviation of all time-domain features\n",
    "std_df = df_df.std()\n",
    "std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e85ebbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3728a581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['min_1', 'max_1', 'mean_1', 'median_1', 'standard deviation_1',\n",
       "       '1st quart_1', '3rd quart_1', 'min_2', 'max_2', 'mean_2', 'median_2',\n",
       "       'standard deviation_2', '1st quart_2', '3rd quart_2', 'min_3', 'max_3',\n",
       "       'mean_3', 'median_3', 'standard deviation_3', '1st quart_3',\n",
       "       '3rd quart_3', 'min_4', 'max_4', 'mean_4', 'median_4',\n",
       "       'standard deviation_4', '1st quart_4', '3rd quart_4', 'min_5', 'max_5',\n",
       "       'mean_5', 'median_5', 'standard deviation_5', '1st quart_5',\n",
       "       '3rd quart_5', 'min_6', 'max_6', 'mean_6', 'median_6',\n",
       "       'standard deviation_6', '1st quart_6', '3rd quart_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_df.columns[1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c97d8102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfidenceInterval(low=8.338966676052632, high=10.821378525935975) for the standard deviation of feature min_1\n",
      "ConfidenceInterval(low=8.340367892592944, high=10.788283441290842) for the standard deviation of feature max_1\n",
      "ConfidenceInterval(low=8.326979203761919, high=10.826438690841279) for the standard deviation of feature mean_1\n",
      "ConfidenceInterval(low=8.314737674426832, high=10.825940086652434) for the standard deviation of feature median_1\n",
      "ConfidenceInterval(low=8.31467871773819, high=10.838965108055413) for the standard deviation of feature standard deviation_1\n",
      "ConfidenceInterval(low=8.31170858164292, high=10.813401607134024) for the standard deviation of feature 1st quart_1\n",
      "ConfidenceInterval(low=8.32404097806381, high=10.820703269146772) for the standard deviation of feature 3rd quart_1\n",
      "ConfidenceInterval(low=8.336051330629722, high=10.8090555602265) for the standard deviation of feature min_2\n",
      "ConfidenceInterval(low=8.330383962187634, high=10.818333854851126) for the standard deviation of feature max_2\n",
      "ConfidenceInterval(low=8.318599207932998, high=10.831387023584803) for the standard deviation of feature mean_2\n",
      "ConfidenceInterval(low=8.33164454381819, high=10.836064525725721) for the standard deviation of feature median_2\n",
      "ConfidenceInterval(low=8.325719355971957, high=10.814743966578545) for the standard deviation of feature standard deviation_2\n",
      "ConfidenceInterval(low=8.305157901710473, high=10.836196582113393) for the standard deviation of feature 1st quart_2\n",
      "ConfidenceInterval(low=8.303793381609283, high=10.818687761032924) for the standard deviation of feature 3rd quart_2\n",
      "ConfidenceInterval(low=8.318189017828416, high=10.804488695917362) for the standard deviation of feature min_3\n",
      "ConfidenceInterval(low=8.352262127696918, high=10.787305451530138) for the standard deviation of feature max_3\n",
      "ConfidenceInterval(low=8.337889259959596, high=10.808288440265015) for the standard deviation of feature mean_3\n",
      "ConfidenceInterval(low=8.330919374729778, high=10.808771120127217) for the standard deviation of feature median_3\n",
      "ConfidenceInterval(low=8.338973354122409, high=10.846401330209655) for the standard deviation of feature standard deviation_3\n",
      "ConfidenceInterval(low=8.34167152211612, high=10.810805955064138) for the standard deviation of feature 1st quart_3\n",
      "ConfidenceInterval(low=8.285103931640787, high=10.817285642332907) for the standard deviation of feature 3rd quart_3\n",
      "ConfidenceInterval(low=8.327740170315192, high=10.811486648971103) for the standard deviation of feature min_4\n",
      "ConfidenceInterval(low=8.318230501642262, high=10.84425939896799) for the standard deviation of feature max_4\n",
      "ConfidenceInterval(low=8.365024943725585, high=10.837903737322277) for the standard deviation of feature mean_4\n",
      "ConfidenceInterval(low=8.31426491631324, high=10.84571066837173) for the standard deviation of feature median_4\n",
      "ConfidenceInterval(low=8.32652193943987, high=10.810675918013388) for the standard deviation of feature standard deviation_4\n",
      "ConfidenceInterval(low=8.317814948521782, high=10.80401461674664) for the standard deviation of feature 1st quart_4\n",
      "ConfidenceInterval(low=8.335160040752857, high=10.809714350610877) for the standard deviation of feature 3rd quart_4\n",
      "ConfidenceInterval(low=8.353035596168477, high=10.824452235930227) for the standard deviation of feature min_5\n",
      "ConfidenceInterval(low=8.312617637187731, high=10.826745011156731) for the standard deviation of feature max_5\n",
      "ConfidenceInterval(low=8.322850714150231, high=10.842870622506359) for the standard deviation of feature mean_5\n",
      "ConfidenceInterval(low=8.307480246891158, high=10.838174471237433) for the standard deviation of feature median_5\n",
      "ConfidenceInterval(low=8.337271682117084, high=10.823480921724082) for the standard deviation of feature standard deviation_5\n",
      "ConfidenceInterval(low=8.335288408504137, high=10.817454282146727) for the standard deviation of feature 1st quart_5\n",
      "ConfidenceInterval(low=8.330068555906854, high=10.820744383953278) for the standard deviation of feature 3rd quart_5\n",
      "ConfidenceInterval(low=8.351008004412403, high=10.84101147976156) for the standard deviation of feature min_6\n",
      "ConfidenceInterval(low=8.308342331749714, high=10.814152924968617) for the standard deviation of feature max_6\n",
      "ConfidenceInterval(low=8.302636642969022, high=10.808520233137118) for the standard deviation of feature mean_6\n",
      "ConfidenceInterval(low=8.312545015633269, high=10.807550542654493) for the standard deviation of feature median_6\n",
      "ConfidenceInterval(low=8.336990837820066, high=10.816434286231681) for the standard deviation of feature standard deviation_6\n",
      "ConfidenceInterval(low=8.331299765047891, high=10.82192051410171) for the standard deviation of feature 1st quart_6\n",
      "ConfidenceInterval(low=8.315704578807116, high=10.822552935894578) for the standard deviation of feature 3rd quart_6\n"
     ]
    }
   ],
   "source": [
    "# reference\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html\n",
    "data_min1 = np.array(df_df['min_1'])\n",
    "data_min1=(data_min1,) # must need to convert the array to be a sequence first\n",
    "rng = np.random.default_rng() # for random state\n",
    "for col in df_df.columns[1::]:\n",
    "    res = bootstrap(data_min1, np.std, confidence_level=0.9,random_state=rng,method='basic')\n",
    "    print(res.confidence_interval,'for the standard deviation of feature',col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad070e2",
   "metadata": {},
   "source": [
    "##### 1 (c) iv The most 'important' three time-domain features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f97970",
   "metadata": {},
   "source": [
    "Mean, median, and standard deviation. \n",
    "\n",
    "With the first two features, we can kinda know what the distribution is like. Is it skewed? etc. With standard deviation, we can know how far it is for every sample to the mean. This will give us a better idea how disperse the data are. Therefore, with these three features, we can imagine the distribution in our head beforehand, which will help us understand the problem better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875e8f6",
   "metadata": {},
   "source": [
    "##### 2. Question from the Book ISLR 3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1c536",
   "metadata": {},
   "source": [
    "##### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2daf28",
   "metadata": {},
   "source": [
    "I think the RSS for the linear regression will be higher than that of cubic regression. The reason being that the cubic regression could result in a overfitting problem on the training dataset albeit the true relationship being linear. The cubic regression model is more flexible, so lower training RSS for the cubic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21667c34",
   "metadata": {},
   "source": [
    "##### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc84f5",
   "metadata": {},
   "source": [
    "Under the testing dataset, RSS will be lower for the linear regression model as the original relationship is linear. Cubic regression will not fit as good as linear regression here, so cubic regression will have a higher RSS on the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e4109",
   "metadata": {},
   "source": [
    "##### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac72690",
   "metadata": {},
   "source": [
    "Still in this case, RSS on the training dataset will be smaller for the cubic regression. Since the model is more flexible, the points from training dataset will fit better, compared to the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d6e49",
   "metadata": {},
   "source": [
    "##### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d6a14",
   "metadata": {},
   "source": [
    "We do not have enough information in this case as we don't know how far the relationship is from linear. We don't know how far the relationship is from cubic regression neither. There is a bias and variance trade off here. The testing RSS will be lower for the model that is closer to the original or true relationship between the predictor and the quantitative response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
